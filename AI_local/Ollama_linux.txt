üêß Instalando e rodando modelos com Ollama no Linux
‚úÖ Passo 1: Instalar o Ollama
Abra o terminal e execute:

curl -fsSL https://ollama.com/install.sh | sh
Esse comando baixa e instala o Ollama no seu sistema.

‚úÖ Passo 2: Rodar um modelo
Por exemplo, para rodar o modelo Llama 3 (7B), execute:

ollama run llama3
Ele vai baixar o modelo na primeira vez (uns ~4-8GB, dependendo do modelo) e depois j√° come√ßa a funcionar.

‚úÖ Modelos que voc√™ pode testar:
Modelo	Descri√ß√£o	Comando
llama3	Meta Llama 3 (bem preciso)	ollama run llama3
mistral	Leve, r√°pido, √≥timo para portugu√™s	ollama run mistral
phi3	Leve, muito bom em racioc√≠nio	ollama run phi3
codellama	Focado em programa√ß√£o	ollama run codellama
llava	Multimodal (imagem + texto)	ollama run llava

üëâ Veja todos os modelos aqui: https://ollama.com/library

‚úÖ Passo 3: Usar o Ollama no Python
Instale a biblioteca:

pip install ollama
Exemplo de c√≥digo Python:

import ollama

response = ollama.chat(
    model='llama3',
    messages=[
        {'role': 'user', 'content': 'Qual a dist√¢ncia da Terra at√© Marte?'}
    ]
)

print(response['message']['content'])
Simples assim. Ele usa a API local que roda na sua m√°quina, sem depender da internet ou da OpenAI.

‚úÖ Passo 4 (opcional): Interface gr√°fica (GUI)
Se quiser algo com interface, pode usar:

LM Studio: https://lmstudio.ai (super amig√°vel e simples)

Ollama WebUI: com projetos como Open WebUI, que d√° uma interface parecida com o ChatGPT.


