‚úÖ 1. Usar modelos menores
Alguns modelos s√£o otimizados para m√°quinas com pouca RAM:

Modelo	RAM m√≠nima	Comando	Descri√ß√£o
phi3:mini	~4GB	ollama run phi3:mini	Muito leve e inteligente
gemma:2b	~4GB	ollama run gemma:2b	Da Google, bem leve
mistral:instruct	~5GB	ollama run mistral:instruct	Equil√≠brio bom
tinyllama	~3GB	ollama run tinyllama	Bem pequeno, respostas simples

‚úÖ Tente primeiro:

ollama run phi3:mini
Ou:

ollama run gemma:2b
Esses rodam em quase qualquer m√°quina.

‚úÖ 2. Usar quantiza√ß√£o mais leve
Por padr√£o, Ollama usa modelos quantizados em Q4_K_M (boa performance e tamanho). Voc√™ pode tentar buscar vers√µes mais compactas, mas o Ollama j√° entrega o melhor equil√≠brio poss√≠vel.

‚úÖ 3. Liberar mem√≥ria RAM
Feche outros programas pesados (Chrome, VSCode, etc.).

Se poss√≠vel, aumente sua swap temporariamente para ajudar na RAM.

Aumentar SWAP (emergencial):

sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
Verifique se funcionou:

free -h
Depois, se quiser remover:

sudo swapoff /swapfile
sudo rm /swapfile
‚ö†Ô∏è Aten√ß√£o: SWAP em disco √© mais lento que RAM, mas permite rodar modelos maiores se for s√≥ para testes.

‚úÖ 4. Usar Interface Web mais leve (LM Studio)
O LM Studio gerencia melhor o consumo, permite rodar at√© modelos maiores com menos crashes, dependendo de como eles fazem gerenciamento de mem√≥ria.

üöÄ Resumo pr√°tico
Se voc√™ rodar:

ollama run phi3:mini